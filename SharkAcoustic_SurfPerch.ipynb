{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatrioHSamudra/Predator-Acoustic-Soundscape/blob/main/SharkAcoustic_SurfPerch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQYSBgFlWolU"
      },
      "source": [
        "# **Leveraging AI to Unpick Predator-Prey Relationship in Coral Reefs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSgZ-3-cW9CQ"
      },
      "source": [
        "# Installing the perch codebase from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3_uETbGWm7C"
      },
      "outputs": [],
      "source": [
        "# As Colab updates its environment, the versions of the packages that are\n",
        "# installed by default change. This can lead to compatibility issues with the\n",
        "# perch package. We resolve this by: i) uninstalling some troublsome packages\n",
        "# which now come as defaults with Colab; ii) installing the Perch package\n",
        "# from a specific commit; iii) forcing the correct versions of some packages\n",
        "# to ensure compatibility with one another.\n",
        "\n",
        "# First: drop packages added by more recent colab env updates which cause errors\n",
        "!pip uninstall -y jax-cuda12-pjrt jax-cuda12-plugin\n",
        "\n",
        "# Second: install the pinned version of the surfperch repo from the specific commit.\n",
        "!pip install git+https://github.com/google-research/perch.git@373253f5887e2964a51f348e107889dadfdcece0\n",
        "\n",
        "# Third: Force-reinstall some troublesome packages to ensure compatibility\n",
        "!pip install --force-reinstall jax==0.4.33 jaxlib==0.4.33 tensorflow==2.15 numpy==1.24.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M8ct8MAXKVu"
      },
      "source": [
        "# Hosting the Data in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EPyh_AAW0Lt"
      },
      "outputs": [],
      "source": [
        "# The necessary pipeline to connect this Colab environment with the Google Drive\n",
        "# folder where we host the data used in this tutorial.\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Create the folder to store the sample data\n",
        "drive_sample_data_folder = '/content/drive/MyDrive/SurfPerch Demo with Calling in Our Corals Data/'\n",
        "if not os.path.exists(drive_sample_data_folder):\n",
        "  os.mkdir(drive_sample_data_folder)\n",
        "\n",
        "# This is the location that this tutorial will use to save data.\n",
        "drive_output_directory = '/content/drive/MyDrive/SurfPerch Demo with Calling in Our Corals Outputs/'\n",
        "if not os.path.exists(drive_output_directory):\n",
        "  os.mkdir(drive_output_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAsGkIH-XYH6"
      },
      "source": [
        "# Copy the Data to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF3g8x-DXQJJ"
      },
      "outputs": [],
      "source": [
        "# Copy the sample data from GCS to Google Drive. If you are accidentally running this again, it is safe to interupt this cell and move on.\n",
        "print('Copying data to Google Drive...')\n",
        "!gsutil -m rsync -r gs://chirp-public-bucket/surfperch/ \"/content/drive/MyDrive/SurfPerch Demo with Calling in Our Corals Data/\" # Folder path reference 1\n",
        "\n",
        "# Check the folders got copied. We need the SurfPerch-model, Australia, Indonesia, Philippines, and Seychelles folders.\n",
        "!echo \"Folders successfully copied:\" && ls -l \"/content/drive/MyDrive/SurfPerch Demo with Calling in Our Corals Data/\" | grep \"^d\" # Folder path reference 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrkHj2ovXjKp"
      },
      "source": [
        "# Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XZastyuXcip"
      },
      "outputs": [],
      "source": [
        "# Import various dependencies, including the relevant modules from the Perch\n",
        "# repository. Note that \"chirp\" is the old name that the Perch team used, so any\n",
        "# chirp modules imported here were installed as part of the Perch repository in\n",
        "# one of the previous cells.\n",
        "\n",
        "import collections\n",
        "from collections import Counter\n",
        "from etils import epath\n",
        "from IPython.display import HTML\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display as ipy_display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from chirp.inference import colab_utils\n",
        "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
        "\n",
        "from chirp import audio_utils\n",
        "from chirp import config_utils\n",
        "from chirp import path_utils\n",
        "from chirp.inference import embed_lib\n",
        "from chirp.inference import models\n",
        "from chirp.inference import tf_examples\n",
        "from chirp.models import metrics\n",
        "from chirp.inference.search import bootstrap\n",
        "from chirp.inference.search import search\n",
        "from chirp.inference.search import display\n",
        "from chirp.inference.classify import classify\n",
        "from chirp.inference.classify import data_lib\n",
        "\n",
        "# If connected to a Colab GPU runtime we should see a GPU listed\n",
        "tf.config.list_physical_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3J4EczFXxHV"
      },
      "source": [
        "# Set the Configuration to Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPpvtVdGXm4x"
      },
      "outputs": [],
      "source": [
        "# Model specific parameters: PLEASE DO NOT CHANGE THE CODE IN THIS CELL.\n",
        "config = config_dict.ConfigDict()\n",
        "embed_fn_config = config_dict.ConfigDict()\n",
        "embed_fn_config.model_key = 'taxonomy_model_tf'\n",
        "model_config = config_dict.ConfigDict()\n",
        "\n",
        "# The size of each \"chunk\" of audio.\n",
        "model_config.window_size_s = 5.0\n",
        "\n",
        "# The hop size\n",
        "model_config.hop_size_s = 5.0\n",
        "\n",
        "# All audio in this tutorial is resampled to 32 kHz.\n",
        "model_config.sample_rate = 32000\n",
        "\n",
        "# The location of the pre-trained model.\n",
        "model_config.model_path = drive_sample_data_folder + 'SurfPerch-model/'\n",
        "\n",
        "# Only write embeddings to reduce size. The Perch codebase supports serializing\n",
        "# a variety of metadata along with the embeddings, but for the purposes of this\n",
        "# tutorial we will not need to make use of those features.\n",
        "embed_fn_config.write_embeddings = True\n",
        "embed_fn_config.write_logits = False\n",
        "embed_fn_config.write_separated_audio = False\n",
        "embed_fn_config.write_raw_audio = False\n",
        "\n",
        "config.embed_fn_config = embed_fn_config\n",
        "embed_fn_config.model_config = model_config\n",
        "\n",
        "# These two settings can be used to break large inputs up into smaller chunks;\n",
        "# this is especially helpful for dealing with long files or very large datasets.\n",
        "# Given free colab has limited resources, you may want to reduce shard_len_s to\n",
        "# 10 to prevent system RAM from becoming overloaded.\n",
        "config.shard_len_s = 60 #\n",
        "config.num_shards_per_file = -1\n",
        "\n",
        "# Number of parent directories to include in the filename. This allows us to\n",
        "# process raw audio that lives in multiple directories.\n",
        "config.embed_fn_config.file_id_depth = 1\n",
        "\n",
        "# If your dataset is large its useful to split the TFRecords across multiple\n",
        "# shards so I/O operations can be parallized.\n",
        "config.tf_record_shards = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-7AzxKdX6lc"
      },
      "source": [
        "# Specify the data (inputs) and results (outputs) directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apyURSowX25D"
      },
      "outputs": [],
      "source": [
        "# @title Hit run on this cell and choose a dataset\n",
        "# Custom CSS to increase label width\n",
        "style = \"\"\"\n",
        "<style>\n",
        ".widget-label {\n",
        "    min-width: 100px !important;\n",
        "}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Apply the CSS\n",
        "ipy_display(HTML(style))\n",
        "\n",
        "# Define the dropdown\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=['Indonesia', 'Australia','Philippines','Seychelles'],\n",
        "    value='Seychelles',\n",
        "    description='Current choice:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Define a function that reacts to changes in the dropdown\n",
        "def on_dataset_change(change):\n",
        "    dataset_choice = change['new']\n",
        "    print(f'Changed dataset to: {dataset_choice}. Now work through the cells below for this dataset.')\n",
        "\n",
        "# Attach the observer to the dropdown\n",
        "dropdown.observe(on_dataset_change, names='value')\n",
        "\n",
        "# Display the dropdown\n",
        "ipy_display(dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r_ucafMX-2x"
      },
      "outputs": [],
      "source": [
        "# Get the chosen datasets name\n",
        "dataset_folder = dropdown.value + '/'\n",
        "\n",
        "# Specify a glob pattern matching any number of wave files.\n",
        "# Use [wW][aA][vV] to match .wav or .WAV files\n",
        "unlabeled_audio_pattern = os.path.join(drive_sample_data_folder, dataset_folder, 'raw_audio/*.[wW][aA][vV]')\n",
        "\n",
        "# Specify a directory where the embeddings will be written.\n",
        "embedding_output_dir = os.path.join(drive_output_directory, dataset_folder, 'raw_embeddings/')\n",
        "if not os.path.exists(embedding_output_dir):\n",
        "  os.makedirs(embedding_output_dir, exist_ok=True)\n",
        "\n",
        "config.output_dir = embedding_output_dir\n",
        "config.source_file_patterns = [unlabeled_audio_pattern]\n",
        "\n",
        "# Create output directory and write the configuration.\n",
        "output_dir = epath.Path(config.output_dir)\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Don't forget to run the dropdown cell above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chAgCHhjYIQl"
      },
      "source": [
        "## Write the configuration to JSON to ensure consistency with later stages of the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPTZT_vrYFJu"
      },
      "outputs": [],
      "source": [
        "# This dumps a config json file next to the embeddings that allows us to reuse\n",
        "# the same embeddings and ensure that we have the correct config that was used\n",
        "# to generate them.\n",
        "embed_lib.maybe_write_config(config, output_dir)\n",
        "\n",
        "# Create SourceInfos configuration, used in sharded computation when computing\n",
        "# embeddings. These source_infos contain metadata about how we're going to\n",
        "# partition the search corpus.  In particular, we're splitting the Powdermill\n",
        "# audio into hundreds of 5s chunks, and the source_infos help us keep track of\n",
        "# which chunk came from which raw audio file.\n",
        "source_infos = embed_lib.create_source_infos(\n",
        "    config.source_file_patterns,\n",
        "    config.num_shards_per_file,\n",
        "    config.shard_len_s)\n",
        "print(f'Constructed {len(source_infos)} source infos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxH57m52YSlW"
      },
      "source": [
        "## Load the Pre-trained Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1G8F4vXYKCg"
      },
      "outputs": [],
      "source": [
        "#@title { vertical-output: true }\n",
        "# Here we're loading our generic Bird Classifier model.\n",
        "# The embed_fn object is a wrapper over the model.\n",
        "embed_fn = embed_lib.EmbedFn(**config.embed_fn_config)\n",
        "print('\\n\\nLoading model(s)...')\n",
        "embed_fn.setup()\n",
        "\n",
        "print('\\n\\nTest-run of model...')\n",
        "z = np.zeros([int(model_config.sample_rate * model_config.window_size_s)])\n",
        "embed_fn.embedding_model.embed(z)\n",
        "print('Setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR13Nc01YZD8"
      },
      "source": [
        "# Generate Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daxJsbVLYcud"
      },
      "source": [
        "## Process the search dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGTBU3W1YV8k"
      },
      "outputs": [],
      "source": [
        "# To reduce the overhead computational resources required and speed up execution\n",
        "# time, we use multiple threads to load the audio before embedding. This tends\n",
        "# to perform faster, but can fail if any audio files are corrupt.\n",
        "\n",
        "# The source_infos variable contains metadata about how to parition the search\n",
        "# corpus.  This step creates an audio_iterator which iterates over the 5 second\n",
        "# chunks of audio.\n",
        "\n",
        "embed_fn.min_audio_s = 1.0\n",
        "record_file = (output_dir / 'embeddings.tfrecord').as_posix()\n",
        "succ, fail = 0, 0\n",
        "\n",
        "audio_loader = lambda fp, offset: audio_utils.load_audio_window(\n",
        "    fp, offset, model_config.sample_rate, config.shard_len_s)\n",
        "audio_iterator = audio_utils.multi_load_audio_window(\n",
        "    audio_loader=audio_loader,\n",
        "    filepaths=[s.filepath for s in source_infos],\n",
        "    offsets=[s.shard_num * s.shard_len_s for s in source_infos],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2NP0wEYh32"
      },
      "source": [
        "## Embed the search dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIM_0PefYfFm"
      },
      "outputs": [],
      "source": [
        "#@title { vertical-output: true }\n",
        "# Embed! This step may take several minutes to run.\n",
        "with tf_examples.EmbeddingsTFRecordMultiWriter(\n",
        "    output_dir=output_dir, num_files=config.tf_record_shards) as file_writer:\n",
        "  for source_info, audio in tqdm.tqdm(\n",
        "      zip(source_infos, audio_iterator), total=len(source_infos)):\n",
        "    if audio.shape[0] < embed_fn.min_audio_s * model_config.sample_rate:\n",
        "      # Ignore short audio.\n",
        "      continue\n",
        "    file_id = source_info.file_id(config.embed_fn_config.file_id_depth)\n",
        "    offset_s = source_info.shard_num * source_info.shard_len_s\n",
        "    example = embed_fn.audio_to_example(file_id, offset_s, audio)\n",
        "    if example is None:\n",
        "      fail += 1\n",
        "      continue\n",
        "    file_writer.write(example.SerializeToString())\n",
        "    succ += 1\n",
        "  file_writer.flush()\n",
        "print(f'\\n\\nSuccessfully processed {succ} source_infos, failed {fail} times.')\n",
        "\n",
        "fns = [fn for fn in output_dir.glob('embeddings-*')]\n",
        "ds = tf.data.TFRecordDataset(fns)\n",
        "parser = tf_examples.get_example_parser()\n",
        "ds = ds.map(parser)\n",
        "for ex in ds.as_numpy_iterator():\n",
        "  print('Recording filename:', ex['filename'])\n",
        "  print('Shape of the embedding:', ex['embedding'].shape)\n",
        "  break\n",
        "\n",
        "# This can take a few moments to get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wp-D2whYs-6"
      },
      "source": [
        "# Audio similarity search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpy19l4lY2Wb"
      },
      "source": [
        "### View the sound types from Seychelles dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGgmlaYRYknp"
      },
      "outputs": [],
      "source": [
        "#@title { vertical-output: true }\n",
        "# Path to cioc target sound folders\n",
        "cioc_sounds = drive_sample_data_folder + dataset_folder + 'cioc_sounds'\n",
        "cioc_sounds_folders = os.listdir(cioc_sounds)\n",
        "\n",
        "# For each target sound folder, find the first audio file as an example\n",
        "example_target_sounds = []\n",
        "for folder in cioc_sounds_folders:\n",
        "  wav_files = [file for file in os.listdir(os.path.join(cioc_sounds, folder)) if file.lower().endswith('.wav')]\n",
        "  example_sound_path = os.path.join(cioc_sounds, folder + '/' + wav_files[0])\n",
        "  example_target_sounds.append(example_sound_path)\n",
        "\n",
        "# Now view each example target sound\n",
        "print('Number of different target sounds: ', len(example_target_sounds))\n",
        "for audio_path in example_target_sounds:\n",
        "  print('Target sound label: ', audio_path.split('/')[-2])\n",
        "  audio = audio_utils.load_audio(audio_path, model_config.sample_rate)\n",
        "  display.plot_audio_melspec(audio, model_config.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iBNPObNY8f2"
      },
      "source": [
        "### Pick a sound type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtTEanTFY-5L"
      },
      "outputs": [],
      "source": [
        "# @title Hit run on this cell and pick a target sound\n",
        "\n",
        "# Ensure the path exists and list directories\n",
        "if os.path.exists(cioc_sounds):\n",
        "    sound_folders = [f for f in os.listdir(cioc_sounds) if os.path.isdir(os.path.join(cioc_sounds, f))]\n",
        "else:\n",
        "    print(\"Path does not exist:\", cioc_sounds)\n",
        "    sound_folders = []\n",
        "\n",
        "# Create and display the dropdown\n",
        "sound_dropdown = widgets.Dropdown(\n",
        "    options=sound_folders,\n",
        "    description='Select sound:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Define a function that reacts to changes in the dropdown\n",
        "def on_sound_change(change):\n",
        "    choice = change['new']\n",
        "    print(f'Changed target sound to: {choice}. Now work through the cells below for this dataset.')\n",
        "\n",
        "# Attach the observer to the dropdown\n",
        "sound_dropdown.observe(on_sound_change, names='value')\n",
        "\n",
        "ipy_display(sound_dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyN1GlL3ZEpl"
      },
      "source": [
        "### Load and view the query audio sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T25wtj77ZI7C"
      },
      "outputs": [],
      "source": [
        "#@title Load and view the query audio sample { vertical-output: true }\n",
        "target_sound = sound_dropdown.value\n",
        "target_classes = [target_sound, 'Unknown']\n",
        "\n",
        "# Select one of the target audio files. Default with 1, but for many sounds\n",
        "# CIOC users surfaced multiple copies which we can leverage. The print\n",
        "# out from this cell will tell you if there are others to choose from.\n",
        "file_index = 1  #@param\n",
        "file_index = file_index -1\n",
        "\n",
        "# Build the folder path\n",
        "target_audio_folder = os.path.join(drive_sample_data_folder, dataset_folder, 'cioc_sounds', target_sound)\n",
        "\n",
        "# Retrieve all .wav files\n",
        "wav_files = [file for file in os.listdir(target_audio_folder) if file.lower().endswith('.wav')]\n",
        "\n",
        "# Print the total number of audio files\n",
        "print(f\"Number of indexed audio files in target sound directory: {len(wav_files)}\")\n",
        "\n",
        "# Validate the user input and select the audio file\n",
        "if 0 <= file_index < len(wav_files):\n",
        "    audio_path = os.path.join(target_audio_folder, wav_files[file_index])\n",
        "    print(f\"Viewing example: {file_index + 1}\")\n",
        "else:\n",
        "    print(\"Invalid file index. Please select a valid index up to and including: \", len(wav_files))\n",
        "    audio_path = None\n",
        "\n",
        "# Assuming the rest of the code executes only if a valid path is selected\n",
        "if audio_path:\n",
        "    audio = audio_utils.load_audio(audio_path, model_config.sample_rate)\n",
        "    display.plot_audio_melspec(audio, model_config.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWDRas51ZMmT"
      },
      "source": [
        "## Select the specific query window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj8yz7C8ZL7f"
      },
      "outputs": [],
      "source": [
        "#@title { vertical-output: true }\n",
        "# If you're audio clip is longer than 5s, adjust start_s to pick your\n",
        "# prefered start time.\n",
        "start_s = 0  #@param\n",
        "\n",
        "# Display the selected window.\n",
        "print('Selected audio window:')\n",
        "st = int(start_s * model_config.sample_rate)\n",
        "end = int(st + model_config.window_size_s * model_config.sample_rate)\n",
        "if end > audio.shape[0]:\n",
        "  end = audio.shape[0]\n",
        "  st = max([0, int(end - model_config.window_size_s * model_config.sample_rate)])\n",
        "audio_window = audio[st:end]\n",
        "display.plot_audio_melspec(audio_window, model_config.sample_rate)\n",
        "\n",
        "query_audio = audio_window\n",
        "sep_outputs = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4dfLVhVZSxi"
      },
      "source": [
        "## Copy CIOC samples to labeled data directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LD07pQkZR-R"
      },
      "outputs": [],
      "source": [
        "# The path to an empty directory where the generated labeled samples will be\n",
        "# placed. Each labeled sample will be placed into a subdirectory corresponding\n",
        "# to the target class that we select for that sample.\n",
        "target_audio_outputs = os.path.join(drive_output_directory, dataset_folder, target_sound + '/labeled_outputs/' + target_sound)\n",
        "os.makedirs(target_audio_outputs, exist_ok=True)\n",
        "\n",
        "# Copy all .wav and .WAV files from target_audio_folder to labeled_data_path\n",
        "for file in os.listdir(target_audio_folder):\n",
        "    if file.lower().endswith('.wav'):\n",
        "        source_path = os.path.join(target_audio_folder, file)\n",
        "        destination_path = os.path.join(target_audio_outputs, file)\n",
        "        shutil.copy2(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLYA-cvJZY5A"
      },
      "source": [
        "### Embed the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSe054EwZW0G"
      },
      "outputs": [],
      "source": [
        "query = query_audio\n",
        "\n",
        "embedded_query = embed_fn.embedding_model.embed(query).embeddings[ :, 0, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpGPOfYkZ02l"
      },
      "source": [
        " ### Create a TensorFlow Dataset (TFDS) wrapper over the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWAWLepUZanQ"
      },
      "outputs": [],
      "source": [
        "# Use the embedded dataset that we created above...\n",
        "bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_path(\n",
        "    embeddings_path=embedding_output_dir,\n",
        "    annotated_path=target_audio_outputs\n",
        ")\n",
        "\n",
        "project_state = bootstrap.BootstrapState(\n",
        "    bootstrap_config, embedding_model=embed_fn.embedding_model)\n",
        "\n",
        "embeddings_ds = project_state.create_embeddings_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WVIeuwNaKZ6"
      },
      "source": [
        "## Run top-k search using a comparison metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnMypvb-aSe4"
      },
      "source": [
        "### Agile modelling step 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU8X2L1vZ268"
      },
      "outputs": [],
      "source": [
        "# Number of search results to capture. top_k = 25 is often a good start,\n",
        "# but we use 10 for brevity in this demo.\n",
        "top_k = 25 # @param {type:\"number\"}\n",
        "\n",
        "# The Perch codebase supports:\n",
        "#  'euclidean', which is the standard euclidean distance\n",
        "#  'cosine', which is the cosine similarity,\n",
        "#  'mip', which is Maximum Inner Product\n",
        "metric = 'euclidean'  #@param['euclidean', 'mip', 'cosine']\n",
        "\n",
        "# Target distance for search results. This lets us try to hone in on a\n",
        "# 'classifier boundary' instead of just looking at the closest matches.\n",
        "# Set to 'None' for raw 'best results' search.\n",
        "target_score = None #@param\n",
        "\n",
        "results, all_scores = search.search_embeddings_parallel(\n",
        "    embeddings_ds, embedded_query,\n",
        "    hop_size_s=model_config.hop_size_s,\n",
        "    top_k=top_k, target_score=target_score, score_fn=metric,\n",
        "    random_sample=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwcZNrfibHOr"
      },
      "source": [
        "### User-in-the-loop data labeling\n",
        "\n",
        "The cell below displays the search results in a user-interface in the following format:\n",
        "* Image: a plot visualization of the audio search result (Mel spectrogram, frequency in Hz over time)\n",
        "* A playback of the audio sample itself\n",
        "* Metrics and metadata: `rank` position, `source file` of the recording segment, `offset_s` (in seconds) from the recording, and the search `score` (i.e. similarity with the query)\n",
        "* Candidate labels for the sample\n",
        "\n",
        "**Instructions to the user:** <br>\n",
        "For each search result presented below, select either the target label or Unknown class. Samples which present difficult cases can be highly valuable to label, but if any cannot be labeled with confidence then its possible to leave them blank.\n",
        "\n",
        "**Quick guide on how to verify vocalisations:**\n",
        "\n",
        "One of the hardest parts of the entire process can be learning to correctly identify the target sounds for yourself. Often this requires a skilled bioacoustician to do well. Many common reef sounds can blur from one clear signal into another, especially tricky pulse, purr and pop vocalizations. When in doubt, take the time to check back against your original target sound. Make use of listening with a good set of earphones or headphones and also visually assessing the spectrogram.\n",
        "\n",
        "For an easy start try the whoop of the Ambon Damselfish in the Australian dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o1oQw4CbR3n"
      },
      "outputs": [],
      "source": [
        "#@title Display the search results for user labeling. { vertical-output: true }\n",
        "display.display_search_results(\n",
        "    project_state=project_state,\n",
        "    results=results,\n",
        "    embedding_sample_rate=model_config.sample_rate,\n",
        "    checkbox_labels=target_classes,\n",
        "    max_workers=5)\n",
        "\n",
        "# Let this cell finish executing before labeling the samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e29vFtC-bfcn"
      },
      "source": [
        "### Choose a target score by plotting a histogram of all distances in the search corpus\n",
        "\n",
        "During the top-k search step above, the Perch code also computed and saved the distances to *every* point in the search corpus. The actual numerical values of these distances are hard to interpret, but the relative values are very useful. In the following cell we plot a histogram of this set of distances to help us conceptualize the geometry of our embedded dataset. This histogram helps us find and tune our values for the `target_score` variable in the top-k search.\n",
        "\n",
        "A typical histogram will appear to fit some vaguely-normal looking distribution, possibly skewed left with a heavy tail. While there is no prescriptive formula for finding useful values of `target_score`, the Perch team has found that good choices tend to lie near the left-hand 'hockey-stick' point of the distribution. For example, in the following histogram, you might try playing with values somewhere in the range of 2.8 to 3.1:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1bLc2XDTqutihg4wJSkCfB2DiT4Dpr4UO\" width=\"40%\" height=\"40%\">\n",
        "\n",
        "These tend to correspond to examples that are faint, or have background noise, or are otherwise not especially obvious. Annotating these examples and adding them to the training set is very important because they help the linear model discriminate better on these less-clear \"boundary\" points. Though take care to make sure your annotations are correct, if in doubt you can leave samples unlabeled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l92G55TtbVgR"
      },
      "outputs": [],
      "source": [
        "#@title { vertical-output: true }\n",
        "# Plot histogram of distances.\n",
        "ys, _, _ = plt.hist(all_scores, bins=128, density=True)\n",
        "hit_scores = [r.score for r in results.search_results]\n",
        "plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
        "            color='r', alpha=0.5)\n",
        "\n",
        "plt.xlabel(metric)\n",
        "plt.ylabel('density')\n",
        "if target_score is not None:\n",
        "  plt.plot([target_score, target_score], [0.0, np.max(ys)], 'r:')\n",
        "  # Compute the proportion of scores < target_score.\n",
        "  hit_percentage = (all_scores < target_score).mean()\n",
        "  print(f'score < target_score percentage : {hit_percentage:5.3f}')\n",
        "min_score = np.min(all_scores)\n",
        "plt.plot([min_score, min_score], [0.0, np.max(ys)], 'g:')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8wRZQ5boAI"
      },
      "source": [
        "### Write the user-annotated search results to file\n",
        "\n",
        "This cell saves the annotations you generated in the previous step to your Google Drives.  It writes data to the `labeled_data_path` location that was specified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-D5GN7FblPR"
      },
      "outputs": [],
      "source": [
        "def write_labeled_data(search_results, labeled_data_path: str, sample_rate: int):\n",
        "  \"\"\"Write labeled results to the labeled data collection.\"\"\"\n",
        "  labeled_data_path = epath.Path(labeled_data_path)\n",
        "  counts = collections.defaultdict(int)\n",
        "  duplicates = collections.defaultdict(int)\n",
        "  for r in search_results:\n",
        "    labels = [ch.description for ch in r.label_widgets if ch.value]\n",
        "    if not labels:\n",
        "      continue\n",
        "    extension = epath.Path(r.filename).suffix\n",
        "    filename = epath.Path(r.filename).name[: -len(extension)]\n",
        "    output_filename = f'{filename}___{r.timestamp_offset}{extension}'\n",
        "    for label in labels:\n",
        "      output_path = labeled_data_path / label\n",
        "      output_path.mkdir(parents=True, exist_ok=True)\n",
        "      output_filepath = epath.Path(output_path / output_filename)\n",
        "      if output_filepath.exists():\n",
        "        duplicates[f'{label}'] += 1\n",
        "        continue\n",
        "      else:\n",
        "        counts[label] += 1\n",
        "      with output_filepath.open('wb') as f:\n",
        "        wavfile.write(f, sample_rate, np.float32(r.audio))\n",
        "  for label, count in counts.items():\n",
        "    print(f'Wrote {count} examples for label {label}')\n",
        "  for label, count in duplicates.items():\n",
        "    print(f'Not saving {count} duplicates for label {label}')\n",
        "\n",
        "labeled_data_path = os.path.join(drive_output_directory, dataset_folder, target_sound + '/labeled_outputs/')\n",
        "write_labeled_data(results, labeled_data_path, model_config.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOQAAes3cmkx"
      },
      "source": [
        "<a name=active-learning></a>\n",
        "## Train a Linear Classifier with active learning 🤖🧠\n",
        "\n",
        "#### Agile modelling step: 2\n",
        "\n",
        "\n",
        "In the agile modelling step 1 above, we labeled samples from the search dataset as matches (by similarity comparison with the queries) for our target classes. If you're satisfied you labelled enough samples during this step, then from here on you will likely want to focus your efforts using the active agile modelling step 2 cells below.\n",
        "\n",
        "We will now train a simple linear model using those bootstrapped (labeled) data points from the search dataset.\n",
        "\n",
        "**Important:** in order to be able to train the linear model, we need several examples from each of the two classes (more is better!). If you encounter an error in this section when training the model, you likely did not generate a sufficient amount of labeled data. Please go back to the [Top-k Search](#top-k-search) section and choose a new value of the `target_score` attribute and label some more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5LIUblPcnDD"
      },
      "outputs": [],
      "source": [
        "# @title Load and embed the search-annotated dataset { vertical-output: true }\n",
        "\n",
        "# Load the training data that is located in the `labeled_data_path` directory.\n",
        "# In that directory there will be folders corresponding to our target labels\n",
        "\n",
        "merged = data_lib.MergedDataset.from_folder_of_folders(\n",
        "    base_dir=labeled_data_path,\n",
        "    embedding_model=project_state.embedding_model,\n",
        "    time_pooling='mean',\n",
        "    load_audio=False,\n",
        "    target_sample_rate=-2,\n",
        "    audio_file_pattern='*',\n",
        "    embedding_config_hash=bootstrap_config.embedding_config_hash(),\n",
        ")\n",
        "\n",
        "# Label distribution\n",
        "lbl_counts = np.sum(merged.data['label_hot'], axis=0)\n",
        "print('num classes :', (lbl_counts > 0).sum())\n",
        "print('mean ex / class :', lbl_counts.sum() / (lbl_counts > 0).sum())\n",
        "print('min ex / class :', (lbl_counts + (lbl_counts == 0) * 1e6).min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rzLmPoocwQW"
      },
      "source": [
        "### Train a simple linear model using the labeled embeddings\n",
        "\n",
        "We use the following hyperparameters which should serve as reasonably performing defaults to train this linear model (classifier):\n",
        "\n",
        "- `batch_size`: 12\n",
        "- `num_epochs`: 128\n",
        "- `num_hiddens`: -1 (to match the dimensions of the embeddings)\n",
        "- `learning_rate`: 0.001\n",
        "\n",
        "Additionally, we compute the following metrics to measure the \"goodness\" of the trained model:\n",
        "- `acc`: overall accuracy\n",
        "- `auc_roc`: AUC ROC, or area under the receiving curve\n",
        "- `cmAP`: mean average precision averaged across species\n",
        "- `maps`: mean average precision for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GYC2mzPcsgm"
      },
      "outputs": [],
      "source": [
        "# Number of random training examples to choose from each class.\n",
        "\n",
        "# Note that if you don't have very many samples you'll need to set\n",
        "# train_ratio=None and train_examples_per_class to a value that is\n",
        "# less than the minimum number of examples you have of each class.\n",
        "\n",
        "# Set exactly one of train_ratio and train_examples_per_class\n",
        "train_ratio = 0.8  #@param\n",
        "train_examples_per_class = None  #@param\n",
        "\n",
        "# Number of random re-trainings. In other words, this value indicates how many\n",
        "# models we will train, each will use a new randomly selected combination of\n",
        "# our labeled samples for training and testing. By training multiple models,\n",
        "# we get a sense of model robustness. Here, we train 3, but feel free to\n",
        "# increase it for added confidence in the model's performance.\n",
        "num_seeds = 3  #@param\n",
        "\n",
        "# Classifier training hyperparams.\n",
        "# These should be good defaults.\n",
        "batch_size = 12\n",
        "num_epochs = 128\n",
        "num_hiddens = -1\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcSM6h6FcxKO"
      },
      "outputs": [],
      "source": [
        "# This cell trains the linear model(s) and outputs some summary statistics for\n",
        "# each model. If you only have num_seeds = 1 then we'll only train a single\n",
        "# model here.\n",
        "metrics = collections.defaultdict(list)\n",
        "for seed in tqdm.tqdm(range(num_seeds)):\n",
        "  if num_hiddens > 0:\n",
        "    model = classify.get_two_layer_model(\n",
        "        num_hiddens, merged.embedding_dim, merged.num_classes)\n",
        "  else:\n",
        "    model = classify.get_linear_model(\n",
        "        merged.embedding_dim, merged.num_classes)\n",
        "  run_metrics = classify.train_embedding_model(\n",
        "      model, merged, train_ratio, train_examples_per_class,\n",
        "      num_epochs, seed, batch_size, learning_rate)\n",
        "  metrics['acc'].append(run_metrics.top1_accuracy)\n",
        "  metrics['auc_roc'].append(run_metrics.auc_roc)\n",
        "  metrics['cmap'].append(run_metrics.cmap_value)\n",
        "  metrics['maps'].append(run_metrics.class_maps)\n",
        "  metrics['test_logits'].append(run_metrics.test_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWN-tQNbc1pt"
      },
      "source": [
        "#### Compute the average metrics and print the model performance\n",
        "\n",
        "In the previous cells, the `num_seeds` param controls how many times we train a model.  Each time we train a model there is some randomness in terms of which data points we choose from our labeled data, as well as some randomness in the model's initialization.  We can get a sense of how robust our classifier is by training multple times and looking at the summary statistics computed by the following cell.  A low `auc_roc` value (ie, less than 0.9 or so) probably indicates that we should generate some more training data.\n",
        "\n",
        "**Note**: If you are new to labeling marine sounds, it can also be tricky to be sure you've labeled everything correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aULROnDczCn"
      },
      "outputs": [],
      "source": [
        "mean_acc = np.mean(metrics['acc'])\n",
        "mean_auc = np.mean(metrics['auc_roc'])\n",
        "mean_cmap = np.mean(metrics['cmap'])\n",
        "# Merge the test_logits into a single array.\n",
        "test_logits = {\n",
        "    k: np.concatenate([logits[k] for logits in metrics['test_logits']])\n",
        "    for k in metrics['test_logits'][0].keys()\n",
        "}\n",
        "\n",
        "print(f'acc:{mean_acc:5.2f}, auc_roc:{mean_auc:5.2f}, cmap:{mean_cmap}')\n",
        "for lbl, auc in zip(merged.labels, run_metrics.class_maps):\n",
        "  if np.isnan(auc):\n",
        "    continue\n",
        "  print(f'\\n{lbl:8s}, auc_roc:{auc:5.2f}')\n",
        "  colab_utils.prstats(f'test_logits({lbl})',\n",
        "                      test_logits[merged.labels.index(lbl)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hr-xiPCdAS5"
      },
      "source": [
        "If you have completed the active learning loop below a few times, and have a high auc_roc score (>0.95 or close to it), then move on to the 'Inference stage' cell below.\n",
        "\n",
        "### The Active Learning Loop: Generating more training examples\n",
        "\n",
        "### Generate new samples using logit scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC0TnqM0c9Sm"
      },
      "outputs": [],
      "source": [
        "# Choose the target class to work with.  This must be one of the values from\n",
        "# the target_classes list above (target_sound or 'Unknown')\n",
        "\n",
        "target_class = target_sound  #@param\n",
        "\n",
        "# Choose a target logit. You can start by setting this to 'None' to get the\n",
        "# highest-logit examples, which should reveal more of the target sound.\n",
        "# Next, selecting 0.0 or the hockey stick bend should reveal samples the model\n",
        "# currently finds difficult - these are very valuable to label.\n",
        "target_logit = none  #@param\n",
        "\n",
        "# Number of results to display.\n",
        "num_results = 10  #@param\n",
        "\n",
        "# Create the embeddings dataset.\n",
        "target_class_idx = merged.labels.index(target_class)\n",
        "results, all_logits = search.classifer_search_embeddings_parallel(\n",
        "    embeddings_classifier=model,\n",
        "    target_index=target_class_idx,\n",
        "    embeddings_dataset=embeddings_ds,\n",
        "    hop_size_s=model_config.hop_size_s,\n",
        "    target_score=target_logit,\n",
        "    top_k=num_results\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFXbngZNdFdP"
      },
      "outputs": [],
      "source": [
        "#@title { vertical-output: true }\n",
        "# Plot the histogram of model logits.\n",
        "_, ys, _ = plt.hist(all_logits, bins=128, density=True)\n",
        "plt.xlabel(f'{target_class} logit')\n",
        "plt.ylabel('density')\n",
        "plt.plot([target_logit, target_logit], [0.0, np.max(ys)], 'r:')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUm7B8-JdLuF"
      },
      "outputs": [],
      "source": [
        "#@title Display results for the target label { vertical-output: true }\n",
        "\n",
        "display_labels = merged.labels\n",
        "\n",
        "extra_labels = []  #@param\n",
        "for label in extra_labels:\n",
        "  if label not in merged.labels:\n",
        "    display_labels += (label,)\n",
        "if 'Unknown' not in merged.labels:\n",
        "  display_labels += ('Unknown',)\n",
        "\n",
        "display.display_search_results(\n",
        "    results=results,\n",
        "    project_state=project_state,\n",
        "    embedding_sample_rate=model_config.sample_rate,\n",
        "    checkbox_labels=target_classes,\n",
        "    max_workers=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65mGI5JwdQ-Z"
      },
      "source": [
        "#### Add selected results to the labeled data\n",
        "\n",
        "As before, once we've annotated the examples from the previous cell, we'll save them in the `labeled_data_dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF1qIOXkdQUo"
      },
      "outputs": [],
      "source": [
        "results.write_labeled_data(labeled_data_path, model_config.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMLy-47Pdiva"
      },
      "source": [
        "Now return to the 'Agile modelling step: 2' cell and work down from here cell by cell once again, until you are satisified with the auc_roc score output by the 'Compute the average metrics' cell.\n",
        "\n",
        "If you are satsified by the score, move on to the final stage below..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C9tCqvbdggx"
      },
      "source": [
        "### Inference stage: write the trained model's classification results\n",
        "\n",
        "Usually the purpose of creating a model in the first place is to bulk-process many hours of raw audio.  In this cell, we'll run our linear model over the entire search corpus.  The output will be a CSV containing the results.  \n",
        "\n",
        "The `threshold` parameter is the minimum logit score that will get recorded (ie, samples wfor which the model had a low confidence are  omitted from the results to ensure high confidence in the final results).  You can tune this score to generate different output CSVs at different confidence scores. Alternatively, if you notice in your results csv that many periods are omitted but you don't want to introduce potential error by lowering the logit threshold, you can increase the strength of logit scores by carefully labeling more data and retraining your classifier using the active learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqX_2aq3dWqT"
      },
      "outputs": [],
      "source": [
        "# { vertical-output: true }\n",
        "\n",
        "threshold = 1.0  #@param\n",
        "output_filepath = os.path.join(drive_output_directory, dataset_folder, target_sound + '/inference_'+ target_sound + '.csv')\n",
        "\n",
        "def classify_batch(batch):\n",
        "  \"\"\"Classify a batch of embeddings.\"\"\"\n",
        "  emb = batch[tf_examples.EMBEDDING]\n",
        "  emb_shape = tf.shape(emb)\n",
        "  flat_emb = tf.reshape(emb, [-1, emb_shape[-1]])\n",
        "  logits = model(flat_emb)\n",
        "  logits = tf.reshape(\n",
        "      logits, [emb_shape[0], emb_shape[1], tf.shape(logits)[-1]])\n",
        "  # Take the maximum logit over channels.\n",
        "  logits = tf.reduce_max(logits, axis=-2)\n",
        "  batch['logits'] = logits\n",
        "  return batch\n",
        "\n",
        "\n",
        "inference_ds = embeddings_ds.map(\n",
        "    classify_batch, num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "all_embeddings = []\n",
        "all_logits = []\n",
        "\n",
        "with open(output_filepath, 'w') as f:\n",
        "  # Write column headers.\n",
        "  headers = ['filename', 'timestamp_s', 'label', 'logit']\n",
        "  f.write(', '.join(headers) + '\\n')\n",
        "  for ex in tqdm.tqdm(inference_ds.as_numpy_iterator()):\n",
        "    all_embeddings.append(ex['embedding'])\n",
        "    all_logits.append(ex['logits'])\n",
        "    for t in range(ex['logits'].shape[0]):\n",
        "      for i, label in enumerate(merged.labels):\n",
        "        if ex['logits'][t, i] > threshold:\n",
        "          offset = ex['timestamp_s'] + t * model_config.hop_size_s\n",
        "\n",
        "          logit = '{:.2f}'.format(ex['logits'][t, i])\n",
        "          row = [ex['filename'].decode('utf-8'),\n",
        "                 '{:.2f}'.format(offset),\n",
        "                 label, logit]\n",
        "          f.write(', '.join(row) + '\\n')\n",
        "\n",
        "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "all_logits = np.concatenate(all_logits, axis=0)\n",
        "print('Saved results to: ', output_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s-87LkrdpEE"
      },
      "source": [
        "<a name=results></a>\n",
        "# Results .csv files\n",
        "\n",
        "With the example datasets used in this demo, we can also compare the occurence of target sounds between the relevant habitat types the data was gathered from.\n",
        "\n",
        "The csv results file contains a list of all filenames and their predicted label. Below is some custom code for the datasets used in this demo which reads the filenames present in the csv and produces plots.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u72NULpCdm-Y"
      },
      "outputs": [],
      "source": [
        "# Find the output folders for each target sound analysed so far\n",
        "sound_folders = os.listdir(os.path.join(drive_output_directory, dataset_folder))\n",
        "if 'raw_embeddings' in sound_folders:\n",
        "    sound_folders.remove('raw_embeddings')\n",
        "\n",
        "# Store results in dict\n",
        "all_results_dict = {}\n",
        "for sound in sound_folders:\n",
        "  # Load results csv\n",
        "  results_csv_path = os.path.join(drive_output_directory,\n",
        "                                  dataset_folder,\n",
        "                                  sound + '/inference_'+ sound + '.csv')\n",
        "  results_df = pd.read_csv(results_csv_path)\n",
        "\n",
        "  # Strip white space\n",
        "  results_df.columns = results_df.columns.str.strip()\n",
        "  results_df = results_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "\n",
        "  # Find the files with the target sound and store their habitat type\n",
        "  target_detected = results_df[results_df['label'] == sound]\n",
        "  source_files = list(target_detected['filename'])\n",
        "  habitat_type = [item[20] for item in source_files] # our data has the habitat type denoted at the 20th character in the filename\n",
        "  habitat_counts = Counter(habitat_type)\n",
        "  habitat_counts_dict = dict(habitat_counts)\n",
        "  all_results_dict[sound] = habitat_counts_dict"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7ZweEgiRYiIgNGEhwS3TO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}